{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a568d99e",
   "metadata": {},
   "source": [
    "### **Imports and configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751d430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chis Bogdan-Mihai\\Work\\UBB\\Sem_5\\KBS\\ai-math-tutor\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "DATASET_NAME = \"eth-nlped/mathdial\"\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "EPOCHS = 3\n",
    "LR = 5e-5\n",
    "\n",
    "PER_DEVICE_TRAIN_BS = 4\n",
    "PER_DEVICE_EVAL_BS = 4\n",
    "GRAD_ACCUM = 4\n",
    "SEED = 42\n",
    "\n",
    "RUN_TUTOR_ONLY = True\n",
    "RUN_MASKED_LOSS = True\n",
    "\n",
    "OUTPUT_DIR_TUTOR_ONLY = \"./gpt2-mathdial-tutor_only\"\n",
    "OUTPUT_DIR_MASKED_LOSS = \"./gpt2-mathdial-masked_loss\"\n",
    "\n",
    "INCLUDE_QUESTION = True\n",
    "\n",
    "USE_FP16 = torch.cuda.is_available()\n",
    "USE_BF16 = False  # set True if you know your GPU supports bf16 well\n",
    "USE_GRADIENT_CHECKPOINTING = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a8dec",
   "metadata": {},
   "source": [
    "### **Load dataset and make test-train split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8641779a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['qid', 'scenario', 'question', 'ground_truth', 'student_incorrect_solution', 'student_profile', 'teacher_described_confusion', 'self-correctness', 'self-typical-confusion', 'self-typical-interactions', 'conversation'],\n",
      "    num_rows: 2035\n",
      "}) Dataset({\n",
      "    features: ['qid', 'scenario', 'question', 'ground_truth', 'student_incorrect_solution', 'student_profile', 'teacher_described_confusion', 'self-correctness', 'self-typical-confusion', 'self-typical-interactions', 'conversation'],\n",
      "    num_rows: 227\n",
      "})\n",
      "dict_keys(['qid', 'scenario', 'question', 'ground_truth', 'student_incorrect_solution', 'student_profile', 'teacher_described_confusion', 'self-correctness', 'self-typical-confusion', 'self-typical-interactions', 'conversation'])\n"
     ]
    }
   ],
   "source": [
    "raw = load_dataset(DATASET_NAME)\n",
    "\n",
    "if \"validation\" in raw:\n",
    "    train_raw = raw[\"train\"]\n",
    "    val_raw = raw[\"validation\"]\n",
    "else:\n",
    "    split = raw[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
    "    train_raw, val_raw = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(train_raw, val_raw)\n",
    "print(train_raw[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b7621",
   "metadata": {},
   "source": [
    "### **Tokenizer + special tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45305fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50262, '<|endoftext|>', '<|endoftext|>')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "REDACT_TOKEN = \"<FINAL_ANSWER_REDACTED>\"\n",
    "SPECIAL_TOKENS = [\"<STUDENT>\", \"<TUTOR>\", \"<PROBLEM>\", \"</PROBLEM>\", REDACT_TOKEN]\n",
    "\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": SPECIAL_TOKENS})\n",
    "\n",
    "# GPT-2 has no pad token by default\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "len(tokenizer), tokenizer.pad_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec6ca91",
   "metadata": {},
   "source": [
    "### **Parsing and final-answer detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c83218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOM = \"|EOM|\"\n",
    "TEACHER_PREFIX = \"Teacher:\"\n",
    "STUDENT_PREFIX = \"Student:\"\n",
    "\n",
    "# B-mode: only strong cues\n",
    "FINAL_CUE_PATTERNS = [\n",
    "    r\"\\bfinal answer\\b\",\n",
    "    r\"\\bthe answer is\\b\",\n",
    "    r\"^\\s*answer\\s*:\\s*\",\n",
    "    r\"^\\s*solution\\s*:\\s*\",\n",
    "    r\"^\\s*final\\s*:\\s*\",\n",
    "]\n",
    "\n",
    "def strip_dialog_act(text: str) -> str:\n",
    "    # e.g. \"(act) text\" -> \"text\"\n",
    "    return re.sub(r\"^\\s*\\([^)]*\\)\\s*\", \"\", text).strip()\n",
    "\n",
    "def is_final_answer_like_B(text: str) -> bool:\n",
    "    t = strip_dialog_act(text).lower()\n",
    "    return any(re.search(p, t, flags=re.IGNORECASE | re.MULTILINE) for p in FINAL_CUE_PATTERNS)\n",
    "\n",
    "def parse_conversation(conv: str) -> List[Tuple[str, str]]:\n",
    "    parts = [p.strip() for p in conv.split(EOM)]\n",
    "    turns: List[Tuple[str, str]] = []\n",
    "    for p in parts:\n",
    "        if not p:\n",
    "            continue\n",
    "        if p.startswith(TEACHER_PREFIX):\n",
    "            turns.append((\"teacher\", p[len(TEACHER_PREFIX):].strip()))\n",
    "        elif p.startswith(STUDENT_PREFIX):\n",
    "            turns.append((\"student\", p[len(STUDENT_PREFIX):].strip()))\n",
    "        else:\n",
    "            # fallback\n",
    "            turns.append((\"student\", p.strip()))\n",
    "    return turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a912395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13312/13312 [00:02<00:00, 5378.47 examples/s]\n",
      "Map: 100%|██████████| 1472/1472 [00:00<00:00, 5218.65 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 13312\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 1472\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_samples_tutor_only(example: Dict[str, Any], include_question: bool) -> List[Dict[str, str]]:\n",
    "    q = (example.get(\"question\") or \"\").strip()\n",
    "    turns = parse_conversation(example[\"conversation\"])\n",
    "\n",
    "    samples = []\n",
    "    history = \"\"\n",
    "    if include_question and q:\n",
    "        history = f\"<PROBLEM>\\n{q}\\n</PROBLEM>\\n\"\n",
    "\n",
    "    for role, text in turns:\n",
    "        if role == \"student\":\n",
    "            history += f\"<STUDENT> {text}\\n\"\n",
    "            continue\n",
    "\n",
    "        # teacher turn\n",
    "        if is_final_answer_like_B(text):\n",
    "            # redact from context and do not use as training target\n",
    "            history += f\"<TUTOR> {REDACT_TOKEN}\\n\"\n",
    "            continue\n",
    "\n",
    "        prompt = history + \"<TUTOR> \"\n",
    "        target = f\"{strip_dialog_act(text)}\\n\"\n",
    "        samples.append({\"text\": prompt + target})\n",
    "        history += f\"<TUTOR> {strip_dialog_act(text)}\\n\"\n",
    "\n",
    "    return samples\n",
    "\n",
    "def explode_tutor_only(ds_in) -> Dataset:\n",
    "    rows = {\"text\": []}\n",
    "    for ex in ds_in:\n",
    "        for s in build_samples_tutor_only(ex, include_question=INCLUDE_QUESTION):\n",
    "            rows[\"text\"].append(s[\"text\"])\n",
    "    return Dataset.from_dict(rows)\n",
    "\n",
    "train_text = explode_tutor_only(train_raw)\n",
    "val_text = explode_tutor_only(val_raw)\n",
    "\n",
    "def tok_tutor_only(batch):\n",
    "    out = tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=False)\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "train_tutor_only = train_text.map(tok_tutor_only, batched=True, remove_columns=train_text.column_names)\n",
    "val_tutor_only = val_text.map(tok_tutor_only, batched=True, remove_columns=val_text.column_names)\n",
    "\n",
    "train_tutor_only, val_tutor_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7e90af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 227/227 [00:00<00:00, 763.65 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 2035\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 227\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_features_masked_loss(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    q = (example.get(\"question\") or \"\").strip()\n",
    "    turns = parse_conversation(example[\"conversation\"])\n",
    "\n",
    "    segments: List[Tuple[str, str, bool]] = []  # (role, text, supervise?)\n",
    "    if INCLUDE_QUESTION and q:\n",
    "        segments.append((\"meta\", f\"<PROBLEM>\\n{q}\\n</PROBLEM>\\n\", False))\n",
    "\n",
    "    for role, text in turns:\n",
    "        if role == \"student\":\n",
    "            segments.append((\"student\", f\"<STUDENT> {text}\\n\", False))\n",
    "        else:\n",
    "            cleaned = strip_dialog_act(text)\n",
    "            if is_final_answer_like_B(cleaned):\n",
    "                segments.append((\"teacher\", f\"<TUTOR> {REDACT_TOKEN}\\n\", False))\n",
    "            else:\n",
    "                segments.append((\"teacher\", f\"<TUTOR> {cleaned}\\n\", True))\n",
    "\n",
    "    input_ids: List[int] = []\n",
    "    labels: List[int] = []\n",
    "\n",
    "    for _, seg_text, supervise in segments:\n",
    "        seg_ids = tokenizer.encode(seg_text, add_special_tokens=False)\n",
    "        if len(input_ids) + len(seg_ids) > MAX_LENGTH:\n",
    "            break\n",
    "        input_ids.extend(seg_ids)\n",
    "        labels.extend(seg_ids if supervise else [-100] * len(seg_ids))\n",
    "\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "train_masked = train_raw.map(build_features_masked_loss, remove_columns=train_raw.column_names)\n",
    "val_masked = val_raw.map(build_features_masked_loss, remove_columns=val_raw.column_names)\n",
    "\n",
    "train_masked, val_masked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bce49732",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CollatorForCausalLM:\n",
    "    tokenizer: AutoTokenizer\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "\n",
    "        def pad(seq, pad_value):\n",
    "            return seq + [pad_value] * (max_len - len(seq))\n",
    "\n",
    "        input_ids = [pad(x[\"input_ids\"], self.tokenizer.pad_token_id) for x in batch]\n",
    "        attention_mask = [pad(x[\"attention_mask\"], 0) for x in batch]\n",
    "        labels = [pad(x[\"labels\"], -100) for x in batch]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "collator = CollatorForCausalLM(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71a17221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fresh_model():\n",
    "    m = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    m.resize_token_embeddings(len(tokenizer))\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        m.gradient_checkpointing_enable()\n",
    "    return m\n",
    "\n",
    "def train_one(model, train_ds, val_ds, out_dir: str):\n",
    "    args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=PER_DEVICE_TRAIN_BS,\n",
    "        per_device_eval_batch_size=PER_DEVICE_EVAL_BS,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.06,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        fp16=USE_FP16,\n",
    "        bf16=USE_BF16,\n",
    "        report_to=\"none\",\n",
    "        seed=SEED,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(out_dir)\n",
    "    tokenizer.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc17da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "C:\\Users\\Chis Bogdan-Mihai\\AppData\\Local\\Temp\\ipykernel_15612\\263564859.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "c:\\Users\\Chis Bogdan-Mihai\\Work\\UBB\\Sem_5\\KBS\\ai-math-tutor\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2496' max='2496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2496/2496 37:12:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.406700</td>\n",
       "      <td>1.548562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.122000</td>\n",
       "      <td>1.435504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.038500</td>\n",
       "      <td>1.411279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chis Bogdan-Mihai\\Work\\UBB\\Sem_5\\KBS\\ai-math-tutor\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Chis Bogdan-Mihai\\Work\\UBB\\Sem_5\\KBS\\ai-math-tutor\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "C:\\Users\\Chis Bogdan-Mihai\\AppData\\Local\\Temp\\ipykernel_15612\\263564859.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "c:\\Users\\Chis Bogdan-Mihai\\Work\\UBB\\Sem_5\\KBS\\ai-math-tutor\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='384' max='384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [384/384 6:25:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.847400</td>\n",
       "      <td>2.433882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.274000</td>\n",
       "      <td>2.225005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.156700</td>\n",
       "      <td>2.197028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chis Bogdan-Mihai\\Work\\UBB\\Sem_5\\KBS\\ai-math-tutor\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\Chis Bogdan-Mihai\\Work\\UBB\\Sem_5\\KBS\\ai-math-tutor\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "if RUN_TUTOR_ONLY:\n",
    "    model_tutor = fresh_model()\n",
    "    train_one(model_tutor, train_tutor_only, val_tutor_only, OUTPUT_DIR_TUTOR_ONLY)\n",
    "\n",
    "if RUN_MASKED_LOSS:\n",
    "    model_masked = fresh_model()\n",
    "    train_one(model_masked, train_masked, val_masked, OUTPUT_DIR_MASKED_LOSS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
